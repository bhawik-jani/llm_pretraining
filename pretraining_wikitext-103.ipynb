{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6e4c88f-950e-4aed-a165-5ac5deb0e62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from transformer import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86ae9532-d7ae-4ee0-b12a-cb534523b1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ab56202-124f-4d33-8d48-d56e47f240cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'Ġhow', 'Ġare', 'Ġyou', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, how are you?\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "385ea426-09a3-4abe-97d8-5f85bb9f0f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([15496,    11,   703,   389,   345,    30])\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer(\"Hello, how are you?\", return_tensors=\"pt\")  # \"pt\" for PyTorch tensors\n",
    "print(encoded[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c70187eb-24c6-4fe4-b05b-dce41426a5dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[15496,    11,   703,   389,   345,    30]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6be224aa-680d-485d-8edf-1cd9a4511840",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, how are you?\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(encoded[\"input_ids\"][0])\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da30f9fc-3080-43d6-938f-4b0c92cac89e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9384c10c-080e-4d40-92f8-5f7abf95f462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WikiText-103 raw dataset\n",
    "with open('wiki.train.raw', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cdd40a26-2d74-4a39-8f26-144c48e31646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n = Valkyria Chronicles III = \\n \\n Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" . \\n The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14105ce7-84a6-4186-8a79-340b73bd5ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (119192782 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "data = tokenizer(text, return_tensors=\"pt\")[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cea6f9e3-1494-4f36-bcc9-8a6b8240a2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75c319df-a9c4-4066-8993-167736d668de",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd3c2c63-a707-4f0f-9d9c-776e7ebb0bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1d67918-2024-4e0b-ac00-3bdc8fa8f1a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119192782"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86d1b565-504b-4a44-b457-abb65b547ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "n = int(0.98*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80414d81-1deb-4ed2-92c7-3ff1eb8d8fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8bc6840-ba69-4834-ae32-64e4b3e8b3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = nn.Embedding(block_size, d_model)\n",
    "        self.transformer = Transformer(num_layers, d_model, nhead, dim_feedforward, dropout=dropout)\n",
    "        self.final = nn.Linear(d_model, vocab_size)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=(2/d_model)**0.5)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=(2/vocab_size)**0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        tok_emb = self.embedding_layer(x) # (B,T,C)\n",
    "        pos_emb = self.position_embedding(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.transformer(x, mask=mask) # (B,T,C)\n",
    "        logits = self.final(x) # (B,T,vocab_size)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "702c83fa-23d8-43af-aabb-4207ebdd0a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 16\n",
    "accumulation_steps = 8\n",
    "block_size = 512\n",
    "num_iters = 30000\n",
    "print_interval = 100\n",
    "val_iters = 8\n",
    "lr = 1e-4\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "d_model = 256\n",
    "nhead = 8\n",
    "num_layers = 8\n",
    "dropout = 0.01\n",
    "dim_feedforward = 2048\n",
    "mask = torch.tril(torch.ones(block_size,block_size)).to(device=device)\n",
    "vocab_size = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "550f6b17-4cd5-4e7a-b878-969a6ec75582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.427857 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = LanguageModel().to(device)\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "422379ef-19c5-4c3f-9443-b578db5f04b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Learning Rate Finder\n",
    "init_lr=1e-7\n",
    "final_lr=10\n",
    "num_steps=100\n",
    "lr_mult = (final_lr / init_lr) ** (1 / num_steps)\n",
    "lr = init_lr\n",
    "optimizer.param_groups[0]['lr'] = lr\n",
    "\n",
    "losses = []\n",
    "lrs = []\n",
    "best_loss = float('inf')\n",
    "\n",
    "for i in range(num_steps):\n",
    "    loss = 0\n",
    "    for _ in range(accumulation_steps):\n",
    "        x, y = get_batch('train')\n",
    "        # Forward pass\n",
    "        logits = model(x)\n",
    "        B, T, C = logits.shape\n",
    "        loss1 = F.cross_entropy(logits.view(B*T, C), y.view(B*T))\n",
    "        loss1.backward()\n",
    "        loss += loss1.item()\n",
    "\n",
    "    # Save loss and learning rate\n",
    "    lrs.append(lr)\n",
    "    losses.append(loss/accumulation_steps)\n",
    "    \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Update learning rate\n",
    "    lr *= lr_mult\n",
    "    optimizer.param_groups[0]['lr'] = lr\n",
    "\n",
    "    # Stop if loss explodes\n",
    "    if loss > best_loss * 4:\n",
    "        break\n",
    "    best_loss = min(best_loss, loss)\n",
    "\n",
    "# Plot Loss vs. Learning Rate\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(lrs, losses)\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Learning Rate\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Learning Rate Finder\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5cfa5d57-4845-44d1-8134-2b3887664033",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba945fc3-8234-4a02-8c69-9c70182dc9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8b441fc-f02b-44c1-9daf-a6810a97e7cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.427857 M parameters\n",
      "step 0: train loss 11.9463, val loss 11.9890\n",
      "step 100: train loss 9.7120, val loss 8.5444\n",
      "step 200: train loss 7.8879, val loss 7.4303\n",
      "step 300: train loss 7.1806, val loss 6.9855\n",
      "step 400: train loss 6.9309, val loss 6.8394\n",
      "step 500: train loss 6.7922, val loss 6.7067\n",
      "step 600: train loss 6.6918, val loss 6.6509\n",
      "step 700: train loss 6.6037, val loss 6.5724\n",
      "step 800: train loss 6.5215, val loss 6.4966\n",
      "step 900: train loss 6.4479, val loss 6.3729\n",
      "step 1000: train loss 6.3569, val loss 6.3653\n",
      "step 1100: train loss 6.2917, val loss 6.3377\n",
      "step 1200: train loss 6.2075, val loss 6.1762\n",
      "step 1300: train loss 6.1872, val loss 6.1508\n",
      "step 1400: train loss 6.1209, val loss 6.0878\n",
      "step 1500: train loss 6.0859, val loss 6.0271\n",
      "step 1600: train loss 6.0358, val loss 6.0156\n",
      "step 1700: train loss 6.0069, val loss 5.9890\n",
      "step 1800: train loss 5.9792, val loss 5.9009\n",
      "step 1900: train loss 5.9226, val loss 5.9227\n",
      "step 2000: train loss 5.9114, val loss 5.9347\n",
      "step 2100: train loss 5.8977, val loss 5.8973\n",
      "step 2200: train loss 5.8402, val loss 5.8698\n",
      "step 2300: train loss 5.8171, val loss 5.8613\n",
      "step 2400: train loss 5.8020, val loss 5.8062\n",
      "step 2500: train loss 5.7741, val loss 5.7003\n",
      "step 2600: train loss 5.7313, val loss 5.7568\n",
      "step 2700: train loss 5.6970, val loss 5.7330\n",
      "step 2800: train loss 5.6751, val loss 5.6863\n",
      "step 2900: train loss 5.6446, val loss 5.6647\n",
      "step 3000: train loss 5.6200, val loss 5.6486\n",
      "step 3100: train loss 5.6043, val loss 5.5825\n",
      "step 3200: train loss 5.5887, val loss 5.5797\n",
      "step 3300: train loss 5.5344, val loss 5.5767\n",
      "step 3400: train loss 5.5140, val loss 5.5182\n",
      "step 3500: train loss 5.5166, val loss 5.4790\n",
      "step 3600: train loss 5.4720, val loss 5.4795\n",
      "step 3700: train loss 5.4706, val loss 5.4855\n",
      "step 3800: train loss 5.4357, val loss 5.4505\n",
      "step 3900: train loss 5.4150, val loss 5.4019\n",
      "step 4000: train loss 5.3960, val loss 5.4154\n",
      "step 4100: train loss 5.3728, val loss 5.3895\n",
      "step 4200: train loss 5.3481, val loss 5.4156\n",
      "step 4300: train loss 5.3271, val loss 5.3486\n",
      "step 4400: train loss 5.2972, val loss 5.3227\n",
      "step 4500: train loss 5.2948, val loss 5.3433\n",
      "step 4600: train loss 5.2512, val loss 5.3164\n",
      "step 4700: train loss 5.2624, val loss 5.2866\n",
      "step 4800: train loss 5.2310, val loss 5.2175\n",
      "step 4900: train loss 5.2114, val loss 5.2366\n",
      "step 5000: train loss 5.1784, val loss 5.2117\n",
      "step 5100: train loss 5.1964, val loss 5.2376\n",
      "step 5200: train loss 5.1827, val loss 5.1155\n",
      "step 5300: train loss 5.1451, val loss 5.1237\n",
      "step 5400: train loss 5.1311, val loss 5.1402\n",
      "step 5500: train loss 5.1141, val loss 5.2272\n",
      "step 5600: train loss 5.0837, val loss 5.1806\n",
      "step 5700: train loss 5.0825, val loss 5.1690\n",
      "step 5800: train loss 5.0923, val loss 5.1453\n",
      "step 5900: train loss 5.0668, val loss 5.1507\n",
      "step 6000: train loss 5.0453, val loss 5.0809\n",
      "step 6100: train loss 5.0303, val loss 5.0709\n",
      "step 6200: train loss 5.0453, val loss 5.0798\n",
      "step 6300: train loss 5.0280, val loss 5.0218\n",
      "step 6400: train loss 5.0243, val loss 5.0764\n",
      "step 6500: train loss 4.9864, val loss 5.0482\n",
      "step 6600: train loss 4.9777, val loss 5.0392\n",
      "step 6700: train loss 4.9875, val loss 5.0365\n",
      "step 6800: train loss 4.9913, val loss 5.0473\n",
      "step 6900: train loss 4.9671, val loss 5.0305\n",
      "step 7000: train loss 4.9421, val loss 4.9308\n",
      "step 7100: train loss 4.9368, val loss 4.9234\n",
      "step 7200: train loss 4.9437, val loss 4.9536\n",
      "step 7300: train loss 4.8960, val loss 5.0444\n",
      "step 7400: train loss 4.9132, val loss 4.8950\n",
      "step 7500: train loss 4.9055, val loss 4.9666\n",
      "step 7600: train loss 4.9035, val loss 4.8820\n",
      "step 7700: train loss 4.8744, val loss 4.9925\n",
      "step 7800: train loss 4.8557, val loss 4.9411\n",
      "step 7900: train loss 4.8499, val loss 4.8449\n",
      "step 8000: train loss 4.8564, val loss 4.8355\n",
      "step 8100: train loss 4.8361, val loss 4.8950\n",
      "step 8200: train loss 4.8074, val loss 4.8347\n",
      "step 8300: train loss 4.8300, val loss 4.8595\n",
      "step 8400: train loss 4.8234, val loss 4.8634\n",
      "step 8500: train loss 4.8136, val loss 4.8136\n",
      "step 8600: train loss 4.7955, val loss 4.8437\n",
      "step 8700: train loss 4.7709, val loss 4.8736\n",
      "step 8800: train loss 4.7784, val loss 4.8554\n",
      "step 8900: train loss 4.7588, val loss 4.7784\n",
      "step 9000: train loss 4.7733, val loss 4.8662\n",
      "step 9100: train loss 4.7635, val loss 4.7946\n",
      "step 9200: train loss 4.7486, val loss 4.7984\n",
      "step 9300: train loss 4.7416, val loss 4.7984\n",
      "step 9400: train loss 4.7469, val loss 4.8370\n",
      "step 9500: train loss 4.7238, val loss 4.7779\n",
      "step 9600: train loss 4.7370, val loss 4.7199\n",
      "step 9700: train loss 4.7259, val loss 4.8086\n",
      "step 9800: train loss 4.6860, val loss 4.7099\n",
      "step 9900: train loss 4.7184, val loss 4.7331\n",
      "step 10000: train loss 4.6834, val loss 4.7647\n",
      "step 10100: train loss 4.6959, val loss 4.8300\n",
      "step 10200: train loss 4.6749, val loss 4.6994\n",
      "step 10300: train loss 4.6838, val loss 4.7671\n",
      "step 10400: train loss 4.6729, val loss 4.6780\n",
      "step 10500: train loss 4.6689, val loss 4.7603\n",
      "step 10600: train loss 4.6361, val loss 4.7034\n",
      "step 10700: train loss 4.6370, val loss 4.7149\n",
      "step 10800: train loss 4.6263, val loss 4.7165\n",
      "step 10900: train loss 4.6688, val loss 4.6617\n",
      "step 11000: train loss 4.6624, val loss 4.7444\n",
      "step 11100: train loss 4.6331, val loss 4.6589\n",
      "step 11200: train loss 4.6456, val loss 4.6278\n",
      "step 11300: train loss 4.6165, val loss 4.7715\n",
      "step 11400: train loss 4.6303, val loss 4.6505\n",
      "step 11500: train loss 4.6031, val loss 4.6924\n",
      "step 11600: train loss 4.6380, val loss 4.7086\n",
      "step 11700: train loss 4.6051, val loss 4.6855\n",
      "step 11800: train loss 4.6042, val loss 4.7202\n",
      "step 11900: train loss 4.5958, val loss 4.5763\n",
      "step 12000: train loss 4.5755, val loss 4.5475\n",
      "step 12100: train loss 4.5676, val loss 4.6350\n",
      "step 12200: train loss 4.5845, val loss 4.5499\n",
      "step 12300: train loss 4.5573, val loss 4.5437\n",
      "step 12400: train loss 4.5504, val loss 4.5605\n",
      "step 12500: train loss 4.5716, val loss 4.6130\n",
      "step 12600: train loss 4.5854, val loss 4.5707\n",
      "step 12700: train loss 4.5601, val loss 4.5700\n",
      "step 12800: train loss 4.5407, val loss 4.6101\n",
      "step 12900: train loss 4.5216, val loss 4.6566\n",
      "step 13000: train loss 4.5265, val loss 4.5916\n",
      "step 13100: train loss 4.5588, val loss 4.6906\n",
      "step 13200: train loss 4.5266, val loss 4.5873\n",
      "step 13300: train loss 4.5410, val loss 4.5831\n",
      "step 13400: train loss 4.4992, val loss 4.5581\n",
      "step 13500: train loss 4.5388, val loss 4.6060\n",
      "step 13600: train loss 4.5155, val loss 4.6001\n",
      "step 13700: train loss 4.5184, val loss 4.5201\n",
      "step 13800: train loss 4.4761, val loss 4.5484\n",
      "step 13900: train loss 4.4984, val loss 4.6108\n",
      "step 14000: train loss 4.5086, val loss 4.5623\n",
      "step 14100: train loss 4.4959, val loss 4.5957\n",
      "step 14200: train loss 4.4783, val loss 4.5260\n",
      "step 14300: train loss 4.4701, val loss 4.4480\n",
      "step 14400: train loss 4.4730, val loss 4.5357\n",
      "step 14500: train loss 4.4759, val loss 4.5828\n",
      "step 14600: train loss 4.4740, val loss 4.5702\n",
      "step 14700: train loss 4.4647, val loss 4.4277\n",
      "step 14800: train loss 4.4605, val loss 4.5166\n",
      "step 14900: train loss 4.4564, val loss 4.5175\n",
      "step 15000: train loss 4.4525, val loss 4.5981\n",
      "step 15100: train loss 4.4806, val loss 4.4643\n",
      "step 15200: train loss 4.4530, val loss 4.5180\n",
      "step 15300: train loss 4.4527, val loss 4.5166\n",
      "step 15400: train loss 4.4412, val loss 4.5015\n",
      "step 15500: train loss 4.4444, val loss 4.5548\n",
      "step 15600: train loss 4.4205, val loss 4.4550\n",
      "step 15700: train loss 4.4501, val loss 4.5626\n",
      "step 15800: train loss 4.3929, val loss 4.5084\n",
      "step 15900: train loss 4.4125, val loss 4.4755\n",
      "step 16000: train loss 4.4147, val loss 4.4653\n",
      "step 16100: train loss 4.4212, val loss 4.5066\n",
      "step 16200: train loss 4.4069, val loss 4.4826\n",
      "step 16300: train loss 4.4027, val loss 4.3947\n",
      "step 16400: train loss 4.4245, val loss 4.4599\n",
      "step 16500: train loss 4.3915, val loss 4.4892\n",
      "step 16600: train loss 4.4335, val loss 4.4925\n",
      "step 16700: train loss 4.3953, val loss 4.4501\n",
      "step 16800: train loss 4.3946, val loss 4.4699\n",
      "step 16900: train loss 4.3866, val loss 4.3512\n",
      "step 17000: train loss 4.3705, val loss 4.4164\n",
      "step 17100: train loss 4.3796, val loss 4.3907\n",
      "step 17200: train loss 4.3719, val loss 4.4722\n",
      "step 17300: train loss 4.3535, val loss 4.5224\n",
      "step 17400: train loss 4.3631, val loss 4.5076\n",
      "step 17500: train loss 4.3541, val loss 4.4496\n",
      "step 17600: train loss 4.3998, val loss 4.4235\n",
      "step 17700: train loss 4.3749, val loss 4.4676\n",
      "step 17800: train loss 4.3459, val loss 4.3433\n",
      "step 17900: train loss 4.3624, val loss 4.4253\n",
      "step 18000: train loss 4.3533, val loss 4.3988\n",
      "step 18100: train loss 4.3576, val loss 4.4024\n",
      "step 18200: train loss 4.3564, val loss 4.3974\n",
      "step 18300: train loss 4.3429, val loss 4.4909\n",
      "step 18400: train loss 4.3262, val loss 4.5002\n",
      "step 18500: train loss 4.3259, val loss 4.4013\n",
      "step 18600: train loss 4.3490, val loss 4.3499\n",
      "step 18700: train loss 4.3219, val loss 4.4365\n",
      "step 18800: train loss 4.3081, val loss 4.3126\n",
      "step 18900: train loss 4.3229, val loss 4.3843\n",
      "step 19000: train loss 4.3252, val loss 4.3477\n",
      "step 19100: train loss 4.3233, val loss 4.3363\n",
      "step 19200: train loss 4.2934, val loss 4.4486\n",
      "step 19300: train loss 4.3294, val loss 4.3613\n",
      "step 19400: train loss 4.3047, val loss 4.3199\n",
      "step 19500: train loss 4.3074, val loss 4.3482\n",
      "step 19600: train loss 4.3240, val loss 4.3835\n",
      "step 19700: train loss 4.3020, val loss 4.4119\n",
      "step 19800: train loss 4.2802, val loss 4.4146\n",
      "step 19900: train loss 4.2989, val loss 4.3394\n",
      "step 20000: train loss 4.3025, val loss 4.3680\n",
      "step 20100: train loss 4.2868, val loss 4.2760\n",
      "step 20200: train loss 4.2992, val loss 4.3970\n",
      "step 20300: train loss 4.2763, val loss 4.3796\n",
      "step 20400: train loss 4.2777, val loss 4.3703\n",
      "step 20500: train loss 4.2795, val loss 4.4002\n",
      "step 20600: train loss 4.2527, val loss 4.3794\n",
      "step 20700: train loss 4.2584, val loss 4.3821\n",
      "step 20800: train loss 4.2608, val loss 4.3237\n",
      "step 20900: train loss 4.2902, val loss 4.3012\n",
      "step 21000: train loss 4.2672, val loss 4.3679\n",
      "step 21100: train loss 4.2541, val loss 4.4792\n",
      "step 21200: train loss 4.2473, val loss 4.3359\n",
      "step 21300: train loss 4.2622, val loss 4.2852\n",
      "step 21400: train loss 4.2521, val loss 4.4018\n",
      "step 21500: train loss 4.2369, val loss 4.3259\n",
      "step 21600: train loss 4.2497, val loss 4.2752\n",
      "step 21700: train loss 4.2648, val loss 4.3055\n",
      "step 21800: train loss 4.2428, val loss 4.3040\n",
      "step 21900: train loss 4.2504, val loss 4.3282\n",
      "step 22000: train loss 4.2499, val loss 4.3208\n",
      "step 22100: train loss 4.2486, val loss 4.3425\n",
      "step 22200: train loss 4.2284, val loss 4.3738\n",
      "step 22300: train loss 4.2456, val loss 4.3751\n",
      "step 22400: train loss 4.2435, val loss 4.3591\n",
      "step 22500: train loss 4.2285, val loss 4.3961\n",
      "step 22600: train loss 4.2305, val loss 4.3870\n",
      "step 22700: train loss 4.2119, val loss 4.3203\n",
      "step 22800: train loss 4.2287, val loss 4.3710\n",
      "step 22900: train loss 4.2215, val loss 4.3144\n",
      "step 23000: train loss 4.2114, val loss 4.4001\n",
      "step 23100: train loss 4.2199, val loss 4.2638\n",
      "step 23200: train loss 4.2166, val loss 4.3144\n",
      "step 23300: train loss 4.2274, val loss 4.2696\n",
      "step 23400: train loss 4.2065, val loss 4.1639\n",
      "step 23500: train loss 4.1955, val loss 4.3192\n",
      "step 23600: train loss 4.1953, val loss 4.2248\n",
      "step 23700: train loss 4.2138, val loss 4.3093\n",
      "step 23800: train loss 4.2071, val loss 4.2855\n",
      "step 23900: train loss 4.1845, val loss 4.2923\n",
      "step 24000: train loss 4.2107, val loss 4.3171\n",
      "step 24100: train loss 4.2089, val loss 4.2351\n",
      "step 24200: train loss 4.1704, val loss 4.2842\n",
      "step 24300: train loss 4.1761, val loss 4.2717\n",
      "step 24400: train loss 4.1994, val loss 4.2408\n",
      "step 24500: train loss 4.1936, val loss 4.3113\n",
      "step 24600: train loss 4.1754, val loss 4.2773\n",
      "step 24700: train loss 4.1861, val loss 4.3208\n",
      "step 24800: train loss 4.1902, val loss 4.3200\n",
      "step 24900: train loss 4.1944, val loss 4.3483\n",
      "step 25000: train loss 4.1682, val loss 4.3108\n",
      "step 25100: train loss 4.1859, val loss 4.2397\n",
      "step 25200: train loss 4.1542, val loss 4.3006\n",
      "step 25300: train loss 4.1672, val loss 4.2770\n",
      "step 25400: train loss 4.1768, val loss 4.2347\n",
      "step 25500: train loss 4.1782, val loss 4.2574\n",
      "step 25600: train loss 4.1685, val loss 4.2697\n",
      "step 25700: train loss 4.1572, val loss 4.2900\n",
      "step 25800: train loss 4.1291, val loss 4.2498\n",
      "step 25900: train loss 4.1489, val loss 4.2518\n",
      "step 26000: train loss 4.1545, val loss 4.2826\n",
      "step 26100: train loss 4.1430, val loss 4.2005\n",
      "step 26200: train loss 4.1606, val loss 4.2208\n",
      "step 26300: train loss 4.1495, val loss 4.2839\n",
      "step 26400: train loss 4.1603, val loss 4.2196\n",
      "step 26500: train loss 4.1413, val loss 4.2485\n",
      "step 26600: train loss 4.1313, val loss 4.3065\n",
      "step 26700: train loss 4.1198, val loss 4.2876\n",
      "step 26800: train loss 4.1406, val loss 4.2765\n",
      "step 26900: train loss 4.1471, val loss 4.2465\n",
      "step 27000: train loss 4.1406, val loss 4.2883\n",
      "step 27100: train loss 4.1193, val loss 4.1881\n",
      "step 27200: train loss 4.1257, val loss 4.2055\n",
      "step 27300: train loss 4.1400, val loss 4.2221\n",
      "step 27400: train loss 4.1332, val loss 4.2202\n",
      "step 27500: train loss 4.1260, val loss 4.3269\n",
      "step 27600: train loss 4.1336, val loss 4.2515\n",
      "step 27700: train loss 4.1233, val loss 4.2408\n",
      "step 27800: train loss 4.1241, val loss 4.2168\n",
      "step 27900: train loss 4.1325, val loss 4.1319\n",
      "step 28000: train loss 4.1250, val loss 4.1884\n",
      "step 28100: train loss 4.1301, val loss 4.2462\n",
      "step 28200: train loss 4.1038, val loss 4.2193\n",
      "step 28300: train loss 4.1171, val loss 4.2396\n",
      "step 28400: train loss 4.1167, val loss 4.2369\n",
      "step 28500: train loss 4.1253, val loss 4.2539\n",
      "step 28600: train loss 4.0892, val loss 4.2354\n",
      "step 28700: train loss 4.0844, val loss 4.1827\n",
      "step 28800: train loss 4.0865, val loss 4.2082\n",
      "step 28900: train loss 4.1240, val loss 4.1681\n",
      "step 29000: train loss 4.0968, val loss 4.1426\n",
      "step 29100: train loss 4.0843, val loss 4.2212\n",
      "step 29200: train loss 4.1083, val loss 4.2392\n",
      "step 29300: train loss 4.1004, val loss 4.1010\n",
      "step 29400: train loss 4.0587, val loss 4.1446\n",
      "step 29500: train loss 4.0890, val loss 4.2109\n",
      "step 29600: train loss 4.0798, val loss 4.1709\n",
      "step 29700: train loss 4.0953, val loss 4.1308\n",
      "step 29800: train loss 4.0856, val loss 4.2623\n",
      "step 29900: train loss 4.0826, val loss 4.1335\n",
      "step 29999: train loss 4.0383, val loss 4.2073\n"
     ]
    }
   ],
   "source": [
    "model = LanguageModel().to(device)\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "train_loss = 0\n",
    "val_loss = 0\n",
    "for n in range(num_iters):\n",
    "    x, y = get_batch('train')\n",
    "    with autocast(device_type=\"cuda\"):\n",
    "        logits = model(x)\n",
    "        B, T, C = logits.shape\n",
    "        loss = F.cross_entropy(logits.view(B*T, C), y.view(B*T)) #/ accumulation_steps\n",
    "    scaler.scale(loss).backward()\n",
    "    if (n + 1) % accumulation_steps == 0:\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "    with torch.no_grad():\n",
    "        train_loss += loss #* accumulation_steps\n",
    "        if (n % print_interval == 0 or n == num_iters - 1):\n",
    "            model.eval()\n",
    "            for _ in range(val_iters):\n",
    "                x, y = get_batch('val')\n",
    "                logits = model(x)\n",
    "                B, T, C = logits.shape\n",
    "                val_loss += F.cross_entropy(logits.view(B*T, C), y.view(B*T))\n",
    "            if n==0:\n",
    "                print(f\"step {n}: train loss {train_loss:.4f}, val loss {val_loss/val_iters:.4f}\")\n",
    "            else:\n",
    "                print(f\"step {n}: train loss {train_loss/print_interval:.4f}, val loss {val_loss/val_iters:.4f}\")\n",
    "            train_loss = 0\n",
    "            val_loss = 0\n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "101e42da-a270-4caa-b079-4dbc0d806b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    \"model_state\": model.state_dict(),\n",
    "    \"optimizer_state\": optimizer.state_dict(),\n",
    "}, \"wiki103_checkpoint.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42bf4eaf-766f-4ddc-a7c6-6bccc8a954cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.427857 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = LanguageModel()\n",
    "checkpoint = torch.load(\"wiki103_checkpoint.pth\")\n",
    "model.load_state_dict(checkpoint[\"model_state\"])\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9c7adc4f-b6ec-4bd5-8996-c839f7ffc758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, x, max_new_tokens, temperature=1):\n",
    "    for _ in range(max_new_tokens):\n",
    "        x_cond = x[:, -block_size:]\n",
    "        logits = model(x_cond)\n",
    "        probs = F.softmax(logits[:, -1, :]/temperature, dim=-1) # (B, C)\n",
    "        # sample from the distribution\n",
    "        x_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "        # append sampled index to the running sequence\n",
    "        x = torch.cat((x, x_next), dim=1) # (B, T+1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f6342a58-c61c-4bf3-8da7-f8fb8cfa808c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      " = = = = New Blue = = = = \n",
      " \n",
      " The White Black mode contains three Lola discs : Troutband Dovictus - Obf3 , eight tailed formed onto traditional English content . \n",
      " \n",
      " = = = Potscape = = = \n",
      " \n",
      " The game focuses on its very unresolved distress phenomenon , characterized by density , range , and often drive potential effects . \n",
      " Technical glass and separating the theoretical routers , such as the previous two discs at the Apple Features ' independent term hardware , is used in Live Productions ' to grind into a space . The chain interface is difficult because it can open cents using digital acids , card sections , at night , and it allows green to be used for local geometry . These , though cut pieces formed from St Pancras .. A portable magnetized capitalist , is required for these titles , while the Webks , many of the old sections in twisted stabilized algorithms , were easily produced in the action and floral interstates VHARD to addition them to R & B 1950 . \n",
      " Camouflage tracks with people with the Matty A + . The Rapture iodide is incorporated in an editorial view that features the yellow cap strip colour and sets of patches used by the fair patterns and visual models . In 1965 , it was issued codonular analysis with one in 2007 reasoning . Requiring the projections was used after smaller scale . The system of all the filters of the silvery @-@ cream 3 are the cartridge @-@ bearing color and canvas rear is used . \n",
      " The track and disc compatible lyrics results are less involved : the packaging can be incorporated into radial @-@ sofa ( range ) . The 1999 edition of Future Network it uses the 50th Fuensee , directed by Gil Stsem Clop , and features the voiced innovation of Arnold Scott , which scenes similar to Escape from Instable Game Analysis by Drake and Greg Van Mask . Acht @-@ Merit @-@ Peck @-@ Mazal Association also uses sites for the Insy @-@ Earth ( VHSB Digital 5 ) in which mixed with dimensions of Turbo ( PfIAL ) , as the bonus system 's volume permits further changing the iPad 2NR fans as well as drawingable difficultyally specresses . A unique addition of critical programming , Pico Computing headquarters is included in menus . \n",
      " \n",
      " = = Reception = = \n",
      " \n",
      " \n",
      " = = =\n"
     ]
    }
   ],
   "source": [
    "context = tokenizer(\" \", return_tensors=\"pt\")[\"input_ids\"].to(device=device)\n",
    "print(tokenizer.decode(generate(model, context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b082039d-8c19-46e9-b1d6-6c488e95cadb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
